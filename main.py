import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# 1. Configurar el modelo que confirmamos que funciona
# Configuramos el modelo con el par√°metro de temperatura
# 0.0 = Muy preciso, casi no var√≠a (ideal para datos t√©cnicos)
# 0.7 - 0.9 = M√°s creativo y fluido (ideal para consultor√≠a o res√∫menes)
llm = ChatGoogleGenerativeAI(
    model="gemini-3-flash-preview", 
    temperature=0.7  # <--- Agrega esta l√≠nea
)
# 2. Leer tu archivo local
with open("datos.txt", "r", encoding="utf-8") as f:
    contenido = f.read()

# 3. Fragmentar el texto (Crucial para RAG)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
fragmentos = text_splitter.split_text(contenido)

print(f"‚úÖ Archivo cargado y dividido en {len(fragmentos)} fragmentos.")

# 4. Pregunta al modelo usando el contexto de tu archivo
pregunta = "¬øQu√© hace exactamente la empresa de Jerem√≠as y por qu√© es relevante?"
prompt = f"""
Act√∫a como un consultor experto en tecnolog√≠a. 
Usa el siguiente contexto para explicar con tus propias palabras la actividad de la empresa:
---
{contenido}
---
Instrucci√≥n: No repitas el texto de forma literal. Expl√≠calo de forma profesional y amena.
"""

response = llm.invoke(prompt)
print("\nü§ñ Respuesta basada en tu archivo:")
print(f"\nü§ñ Respuesta: {response.content[0]['text']}")